{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIR-zVLCczW7"
   },
   "source": [
    "# Set up Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9r54JozycnVz"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "        .master('local[*]')\n",
    "        .appName('Intro to Spark')\n",
    "        .config('spark.ui.port', '4050')\n",
    "        .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuwZsoPDBAX-"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1rhgwz8BhjB"
   },
   "source": [
    "Before starting with the exercises we need to upload the files `people.csv` and `department.csv` in the root folder. These files are necessary to create the dataframes `people_df` and `department_df`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCFCpplZrf0q"
   },
   "outputs": [],
   "source": [
    "people_df = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\n",
    "assert people_df.count() == 1000\n",
    "print(\"File people.csv correctly uploaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyxR7BFGCftT"
   },
   "outputs": [],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Wp4zJbYCh6d"
   },
   "outputs": [],
   "source": [
    "people_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk1oPYZQXkp2"
   },
   "outputs": [],
   "source": [
    "department_df = spark.read.csv(\"department.csv\", header=True, inferSchema=True)\n",
    "assert department_df.count() == 900\n",
    "print(\"File department.csv correctly uploaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrCtmqYVX3oR"
   },
   "outputs": [],
   "source": [
    "department_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RYgHm1fX8XY"
   },
   "outputs": [],
   "source": [
    "department_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rvJDHOwDKI5x"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74BTHJkRilHT"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-I7TIovio52"
   },
   "source": [
    "Create the dataframe `female_df` from the dataframe `people_df`, by selecting the columns `id` and `gender` and filtering the gender column for the value `Female`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "37CNC0cFi_hR"
   },
   "outputs": [],
   "source": [
    "female_df = people_df.select('id', 'gender').filter(col(\"gender\") == \"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3W8i2Qm9jTr7"
   },
   "outputs": [],
   "source": [
    "assert female_df.schema.fieldNames() == ['id', 'gender'] \n",
    "assert female_df.count() == 455\n",
    "print(\"Exercise 1 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nbCjYrSCsEy"
   },
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj1u7tbnIjCF"
   },
   "source": [
    "Create the dataframe`full_address_df` from the dataframe `people_df`, by selecting the columns `city` and `country` and adding a new column `full_address`, which has to be created by concatenating the columns `city` and `country`, separated by a blank space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sGYzn0lxJOUa"
   },
   "outputs": [],
   "source": [
    "full_address_df = people_df.select(col('city'), col('country'), concat_ws(' ', col('city'), col('country')).alias('full_address'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqF56CIqJYTk"
   },
   "outputs": [],
   "source": [
    "assert full_address_df.schema.fieldNames() == ['city', 'country', 'full_address'] \n",
    "assert full_address_df.filter(col('city') == 'Haapsalu').select('full_address').collect()[0][0] == 'Haapsalu Estonia'\n",
    "print(\"Exercise 2 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDNj_8zBnP0Q"
   },
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2ckBccnSGY"
   },
   "source": [
    "Create the dataframe `ethereum_df` from the dataframe `people_df` by selecting the column `ethereum_address` and applying a filter which keeps only the rows that start with `0x1`, `0x2` and `0x3`.\n",
    "\n",
    "Then add a new column `newCol`, with these conditions:\n",
    "\n",
    "1.   If `ethereum_address` starts with `0x1` put `a`\n",
    "2.   Se `ethereum_address` starts with `0x2` put `b`\n",
    "3.   Se `ethereum_address` starts with `0x3` put `c`\n",
    "\n",
    "In the `ethereum_df` dataframe must appear only the columns `ethereum_address` and `newCol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "If566-M3zz4t"
   },
   "outputs": [],
   "source": [
    "ethereum_df = (people_df.select('ethereum_address', substring(col('ethereum_address'), 0, 3).alias('temp')) \n",
    "                    .filter(col('temp').isin('0x1', '0x2', '0x3')) \n",
    "                    .withColumn('newCol', when(col('temp') == '0x1', 'a').when(col('temp') == '0x2', 'b').otherwise('c')) \n",
    "                    .drop('temp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ka57ZVi5hpP"
   },
   "outputs": [],
   "source": [
    "assert ethereum_df.schema.fieldNames() == ['ethereum_address', 'newCol'] \n",
    "assert ethereum_df.count() == 183\n",
    "row = ethereum_df.filter(col('ethereum_address') == '0x1b907715611571700163387611121552d1ad1c9e').collect()[0]\n",
    "assert row['newCol'] == 'a'\n",
    "print(\"Exercise 3 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm9e4fG6spNL"
   },
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9a-gOVrssQM"
   },
   "source": [
    "Create a new dataframe `sql_ethereum_df` from the dataframe `people_df` by creating a column `newCol` with the same conditions of the previous exercise, \n",
    "\n",
    "then we need to perform a substring on `newCol` and apply a filter that keeps only the record equal to `0x1`, `0x2` and `0x3`.\n",
    "\n",
    "Then we need to count the rows for each value of `newCol`` (`0x1`, `0x2` and `0x3`) in a new column `count`.\n",
    "\n",
    "For this exercise we will make use of `Spark SQL`.\n",
    "\n",
    "The dataframe `sql_ethereum_df` must have only the columns `newCol` and `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "sTNgyoA-tXIZ"
   },
   "outputs": [],
   "source": [
    "people_df.createOrReplaceTempView('people_df')\n",
    "sql_ethereum_df = (spark.sql(\"\"\"\n",
    "                      SELECT \n",
    "                          left(ethereum_address,3) AS newCol,\n",
    "                          count(*) AS count\n",
    "                      FROM people_df\n",
    "                      WHERE left(ethereum_address,3) IN ('0x1','0x2','0x3')\n",
    "                      GROUP BY newCol\n",
    "                  \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7P0izvLJtbEv"
   },
   "outputs": [],
   "source": [
    "assert sql_ethereum_df.schema.fieldNames() == ['newCol', 'count'] \n",
    "assert sql_ethereum_df.count() == 3\n",
    "row = sql_ethereum_df.filter(col('newCol') == '0x1').collect()[0]\n",
    "assert row['count'] == 66\n",
    "print(\"Exercise 4 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnt7nNNzQzRN"
   },
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUIhtWjdQ2B-"
   },
   "source": [
    "Let's create a new dataframe `male_df` from the `people_df` dataframe by filtering the records with the column `gender` equal to `Male`.\n",
    "\n",
    "Then let's create a new column `count` which contains the counts of the records per `country` and at finally order the rows by `count` in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "aPpN-uIFRoE9"
   },
   "outputs": [],
   "source": [
    "male_df = (people_df.filter(col('gender') == 'Male')\n",
    "                .groupBy('country')\n",
    "                .agg(count(\"id\").alias(\"count\"))\n",
    "                .orderBy(\"count\", ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrLKWtWDW70L"
   },
   "outputs": [],
   "source": [
    "assert male_df.schema.fieldNames() == ['country', 'count'] \n",
    "row_max =  male_df.collect()[0]\n",
    "assert row_max['country'] == 'China'\n",
    "assert row_max['count'] == 89\n",
    "print(\"Exercise 5 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOAO20uAW9mo"
   },
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYrlIOIqdcnA"
   },
   "source": [
    "Create a new dataframe `people_dept_df` by performing an inner join on the dataframes `people_df` and `department_df` on the column `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "UJXjEIB_XAzc"
   },
   "outputs": [],
   "source": [
    "people_dept_df = people_df.join(department_df, 'id', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qqRn0zqfUUD"
   },
   "outputs": [],
   "source": [
    "assert people_dept_df.count() == 900\n",
    "assert people_dept_df.schema.fieldNames() == ['id', 'first_name', 'last_name', 'email', 'gender', 'ip_address', 'city', 'country', 'ethereum_address', 'department']\n",
    "print(\"Exercise 6 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB1t4flHf8Ao"
   },
   "source": [
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogjZar58f-qH"
   },
   "source": [
    "Create a new dataframe `count_people_dept_df` from the `people_dept_df` by performing a group by on the `department` column and counting the number of people for each department.\n",
    "\n",
    "The column created has to be called `count`.\n",
    "\n",
    "Then let's apply a filter and keep only the records with a count greater then 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "9UmrIBebgLaz"
   },
   "outputs": [],
   "source": [
    "count_people_dept_df = people_dept_df.groupBy('department').agg(count(\"id\").alias(\"count\")).filter(col('count') > 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtq5p50zhSd3"
   },
   "outputs": [],
   "source": [
    "assert count_people_dept_df.count() == 1\n",
    "assert count_people_dept_df.schema.fieldNames() == ['department', 'count']\n",
    "row = count_people_dept_df.collect()[0]\n",
    "assert row['count'] == 203\n",
    "print(\"Exercise 7 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2D-F8WEjg3X"
   },
   "source": [
    "## Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIWT-eHdjmyf"
   },
   "source": [
    "Let's repeat the steps of the previous exercise by using `Spark SQL` and creating the `sql_count_people_dept_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "jNVjQNTIq6b2"
   },
   "outputs": [],
   "source": [
    "people_dept_df.createOrReplaceTempView(\"people_dept_df\")\n",
    "sql_count_people_dept_df = (spark.sql(\"\"\"\n",
    "                              SELECT\n",
    "                                  department,\n",
    "                                  count(*) AS count\n",
    "                              FROM people_dept_df\n",
    "                              GROUP BY department\n",
    "                              HAVING count > 200\n",
    "                            \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6aCPpLiqrBU"
   },
   "outputs": [],
   "source": [
    "assert sql_count_people_dept_df.count() == 1\n",
    "assert sql_count_people_dept_df.schema.fieldNames() == ['department', 'count']\n",
    "row = sql_count_people_dept_df.collect()[0]\n",
    "assert row['count'] == 203\n",
    "print(\"Exercise 8 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIKw6veCPyOt"
   },
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B1C5SfUP1Ji"
   },
   "source": [
    "Let's create the dataframe `ip_address_per_country_df` from the `people_df` dataframe by applying a filter on the `ip_address` column and keeping only the ip addresses which start with `21`.\n",
    "\n",
    "Afterwards let's calculate which country has the greatest number of records. \n",
    "\n",
    "Let's make use of `pyspark.sql.Column.like`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "deFa_wcAQaYX"
   },
   "outputs": [],
   "source": [
    "ip_address_per_country_df = (people_df.filter(col('ip_address').like('21%'))\n",
    "        .groupBy('country')\n",
    "        .agg(count(\"ip_address\").alias(\"count\"))\n",
    "        .orderBy('count', ascending = False)\n",
    "        .limit(1))                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea4BMtLDRkMi"
   },
   "outputs": [],
   "source": [
    "assert ip_address_per_country_df.schema.fieldNames() == ['country', 'count']\n",
    "row = ip_address_per_country_df.filter(col('country') == 'Indonesia').collect()[0]\n",
    "assert row['count'] == 7\n",
    "print(\"Exercise 9 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcWmM5edT-OS"
   },
   "source": [
    "## Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52zWn9j5UAaC"
   },
   "source": [
    "Let's create the `email_per_gender_df` dataframe from the `people_df` dataframe by applying a filter on the `email` column and keeping only the records which end with `.com`.\n",
    "\n",
    "Then let's group by the column `gender` and perform a count thus creating a column called `count`.\n",
    "\n",
    "Finally let's keep only the row with the greatest count.\n",
    "\n",
    "Let's use `pyspark.sql.Column.rlike`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3mDEgX2U30d"
   },
   "outputs": [],
   "source": [
    "email_per_gender_df = (people_df.filter(col('email').rlike('.com$'))\n",
    "                        .groupBy('gender')\n",
    "                        .agg(count(\"ip_address\").alias(\"count\"))\n",
    "                        .orderBy('count', ascending = False)\n",
    "                        .limit(1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aggnSJUXghr"
   },
   "outputs": [],
   "source": [
    "assert email_per_gender_df.count() == 1\n",
    "assert email_per_gender_df.schema.fieldNames() == ['gender', 'count']\n",
    "row = email_per_gender_df.collect()[0]\n",
    "assert row['gender'] == 'Female'\n",
    "assert row['count'] == 275\n",
    "print(\"Exercise 10 passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOEjjgtv8Sx2N8H7yoCpias",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
