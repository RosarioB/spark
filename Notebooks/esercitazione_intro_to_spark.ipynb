{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIR-zVLCczW7"
   },
   "source": [
    "# Set up Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9r54JozycnVz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupiter-pyspark:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Intro to Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffffb395f250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "        .master('local[*]')\n",
    "        .appName('Intro to Spark')\n",
    "        .config('spark.ui.port', '4050')\n",
    "        .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuwZsoPDBAX-"
   },
   "source": [
    "# Esercitazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1rhgwz8BhjB"
   },
   "source": [
    "Prima di iniziare con l'esercitazione carichiamo i file `people.csv` e `department.csv` nella cartella `content` e creiamo da essi il dataframe `people_df` e `department_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gCFCpplZrf0q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File people.csv caricato correttamente!\n"
     ]
    }
   ],
   "source": [
    "people_df = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\n",
    "assert people_df.count() == 1000\n",
    "print(\"File people.csv caricato correttamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_CTRBkyLwBc"
   },
   "source": [
    "Diamo un'occhiata a com'è composto il dataframe `people_df`, facendo uno `show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RyxR7BFGCftT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+--------------------+----------+---------------+-----------------+-------------+--------------------+\n",
      "| id|first_name|    last_name|               email|    gender|     ip_address|             city|      country|    ethereum_address|\n",
      "+---+----------+-------------+--------------------+----------+---------------+-----------------+-------------+--------------------+\n",
      "|  1|    Anatol|        Shave|      ashave0@a8.net|      Male|   32.94.154.64|  General Ramírez|    Argentina|0x1b9077156115717...|\n",
      "|  2|     Marys|   Venediktov|mvenediktov1@nps.gov|    Female| 146.154.160.38|         Haapsalu|      Estonia|0xc7adcf05e5fe5cf...|\n",
      "|  3|    Irvine|        Gaber|igaber2@slashdot.org|      Male|234.128.172.255|   Nyzhnya Krynka|      Ukraine|0x57a55596e577f2e...|\n",
      "|  4|  Kristina|    Skitterel|kskitterel3@opens...|    Female|   105.61.14.18|        Mnelalete|    Indonesia|0x05178ef48e1bc63...|\n",
      "|  5|  Modestia|      Minster|mminster4@sakura....|    Female| 135.235.86.107|      Dobropillya|      Ukraine|0xa5fda6b26484ebb...|\n",
      "|  6|    Murray|        Huddy|     mhuddy5@ask.com|      Male| 173.180.152.76|Ban Huai Thalaeng|     Thailand|0x36925be7427e785...|\n",
      "|  7|     Vince|      Shouler|vshouler6@google....|      Male| 216.124.163.72|        Kivijärvi|      Finland|0xe3b2bd5b0ca5f39...|\n",
      "|  8|    Rainer|   Jachimczak|rjachimczak7@lulu...|      Male|142.127.159.222|          Shuigou|        China|0x58b99f8c7d1c757...|\n",
      "|  9|     Afton|      Pidcock|apidcock8@purevol...|    Female|  126.98.112.89|      Shakhtars’k|      Ukraine|0x9036f790b972b10...|\n",
      "| 10|Kristoforo|      Bransby| kbransby9@tmall.com|      Male|  211.186.238.0|           Kinéta|       Greece|0x56ecb54aa6b4997...|\n",
      "| 11|    Tobiah|        Mavin|  tmavina@cpanel.net|      Male|207.147.163.184|           Gon’ba|       Russia|0x8d86f779fbaacad...|\n",
      "| 12|      Wade|    Kilgannon|wkilgannonb@kicks...|      Male|  148.140.10.33|           Lyubar|      Ukraine|0x8be9ac3c3d87ca5...|\n",
      "| 13|     Felic|    Farnfield|ffarnfieldc@archi...|      Male|  146.241.54.85|        Kertasari|    Indonesia|0xc1a5aaa9df6bcfe...|\n",
      "| 14|    Xavier|Litherborough|xlitherboroughd@d...|      Male|  12.145.172.42|        Lancaster|United States|0xd12e94143be638d...|\n",
      "| 15|    Breena|    Barringer|bbarringere@natio...|    Female| 236.58.255.214|           Hyères|       France|0x2d529d36758fe7b...|\n",
      "| 16|    Granny|     Mulmuray|gmulmurayf@wunder...|Polygender| 150.76.185.175|          Nikopol|     Bulgaria|0x961d9bc8b5d8976...|\n",
      "| 17|   Vasilis|      Grundon|   vgrundong@gnu.org|   Agender|  225.241.10.30|          Ludvika|       Sweden|0x1b807b9056478c4...|\n",
      "| 18|      Hunt|       Arnold|  harnoldh@google.es|      Male|  58.125.75.190|           Batken|   Kyrgyzstan|0xf5a09fe6db7b2e9...|\n",
      "| 19|     Dwain|        Dauby|ddaubyi@yolasite.com|      Male| 189.121.213.32|    Somerset East| South Africa|0xc96532a7a98727b...|\n",
      "| 20|      Arny|      Egerton| aegertonj@slate.com|      Male| 145.96.208.117|       Rejanegara|    Indonesia|0x1ded8db87b30e8b...|\n",
      "+---+----------+-------------+--------------------+----------+---------------+-----------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHEMwcuJL23I"
   },
   "source": [
    "Diamo un'occhiata allo schema del dataframe `people_df` facendo un `printSchema`. In questo modo potremo conoscere le colonne che compongono il dataframe, i relativi tipi e se possono o meno contenere valori null (nullable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4Wp4zJbYCh6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- ethereum_address: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yk1oPYZQXkp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File department.csv caricato correttamente!\n"
     ]
    }
   ],
   "source": [
    "department_df = spark.read.csv(\"department.csv\", header=True, inferSchema=True)\n",
    "assert department_df.count() == 900\n",
    "print(\"File department.csv caricato correttamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GrCtmqYVX3oR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|    department|\n",
      "+---+--------------+\n",
      "|  1|     Marketing|\n",
      "|  2|     Marketing|\n",
      "|  3|Administration|\n",
      "|  4|Administration|\n",
      "|  5|            IT|\n",
      "|  6|Administration|\n",
      "|  7|       Finance|\n",
      "|  8|     Marketing|\n",
      "|  9|         Sales|\n",
      "| 10|            IT|\n",
      "| 11|Administration|\n",
      "| 12|            IT|\n",
      "| 13|            IT|\n",
      "| 14|       Finance|\n",
      "| 15|       Finance|\n",
      "| 16|       Finance|\n",
      "| 17|Administration|\n",
      "| 18|       Finance|\n",
      "| 19|         Sales|\n",
      "| 20|         Sales|\n",
      "+---+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5RYgHm1fX8XY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS7W-rDALkIG"
   },
   "source": [
    "Eseguiamo questa cella per importare tutte le funzioni che ci serviranno per lavorare con i dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rvJDHOwDKI5x"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74BTHJkRilHT"
   },
   "source": [
    "# Esercizio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-I7TIovio52"
   },
   "source": [
    "Dal dataframe `people_df` selezioniamo le colonne `id` e `gender` e filtriamo per la colonna gender = Female, creando il dataframe `female_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "37CNC0cFi_hR"
   },
   "outputs": [],
   "source": [
    "female_df = people_df.select('id', 'gender').filter(col(\"gender\") == \"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3W8i2Qm9jTr7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 1 superato!\n"
     ]
    }
   ],
   "source": [
    "assert female_df.schema.fieldNames() == ['id', 'gender'] \n",
    "assert female_df.count() == 455\n",
    "print(\"Esercizio 1 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nbCjYrSCsEy"
   },
   "source": [
    "# Esercizio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj1u7tbnIjCF"
   },
   "source": [
    "Selezioniamo dal dataframe `people_df` le colonne `city` e `country` e aggiungiamo una nuova colonna `full_address` che sia il risultato della concatenazione delle colonne `city` e `country`, separate da uno spazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sGYzn0lxJOUa"
   },
   "outputs": [],
   "source": [
    "full_address_df = people_df.select(col('city'), col('country'), concat_ws(' ', col('city'), col('country')).alias('full_address'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rqF56CIqJYTk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 2 superato!\n"
     ]
    }
   ],
   "source": [
    "assert full_address_df.schema.fieldNames() == ['city', 'country', 'full_address'] \n",
    "assert full_address_df.filter(col('city') == 'Haapsalu').select('full_address').collect()[0][0] == 'Haapsalu Estonia'\n",
    "print(\"Esercizio 2 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDNj_8zBnP0Q"
   },
   "source": [
    "# Esercizio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2ckBccnSGY"
   },
   "source": [
    "Dal dataframe `people_df` selezioniamo la colonna `ethereum_address` e filtriamo i record che iniziano con `0x1`, `0x2` e `0x3`.\n",
    "\n",
    "Aggiungiamo poi una colonna `newCol`, popolata così:\n",
    "\n",
    "\n",
    "1.   Se `ethereum_address` inizia per 0x1 metto a\n",
    "2.   Se `ethereum_address` inizia per 0x2 metto b\n",
    "3.   Se `ethereum_address` inizia per 0x3 metto c\n",
    "\n",
    "Il daframe finale deve solo le colonne `ethereum_address` e `newCol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "If566-M3zz4t"
   },
   "outputs": [],
   "source": [
    "ethereum_df = (people_df.select('ethereum_address', substring(col('ethereum_address'), 0, 3).alias('temp')) \n",
    "                    .filter(col('temp').isin('0x1', '0x2', '0x3')) \n",
    "                    .withColumn('newCol', when(col('temp') == '0x1', 'a').when(col('temp') == '0x2', 'b').otherwise('c')) \n",
    "                    .drop('temp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3Ka57ZVi5hpP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 3 superato!\n"
     ]
    }
   ],
   "source": [
    "assert ethereum_df.schema.fieldNames() == ['ethereum_address', 'newCol'] \n",
    "assert ethereum_df.count() == 183\n",
    "row = ethereum_df.filter(col('ethereum_address') == '0x1b907715611571700163387611121552d1ad1c9e').collect()[0]\n",
    "assert row['newCol'] == 'a'\n",
    "print(\"Esercizio 3 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm9e4fG6spNL"
   },
   "source": [
    "# Esercizio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9a-gOVrssQM"
   },
   "source": [
    "Dal dataframe `people_df` creiamo la colonna `newCol`come prima, facendo la substring su `ethereum_address` e prendiamo solo i record inclusi in `0x1`, `0x2` e `0x3`.\n",
    "\n",
    "Poi calcoliamo quante righe ci sono per ogni casistica, in una nuova colonna `count`.\n",
    "\n",
    "Per questo esercizio usiamo  `Spark SQL`.\n",
    "\n",
    "Ricordiamo che prima di usare `spark.sql` e fare la query è necessario registrare il dataframe `people_df` col metodo `createOrReplaceTempView`.\n",
    "\n",
    "Mi aspetto che il dataframe finale abbia solo due colonne `newCol` e `count`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "sTNgyoA-tXIZ"
   },
   "outputs": [],
   "source": [
    "people_df.createOrReplaceTempView('people_df')\n",
    "sql_ethereum_df = (spark.sql(\"\"\"\n",
    "                      SELECT \n",
    "                          left(ethereum_address,3) AS newCol,\n",
    "                          count(*) AS count\n",
    "                      FROM people_df\n",
    "                      WHERE left(ethereum_address,3) IN ('0x1','0x2','0x3')\n",
    "                      GROUP BY newCol\n",
    "                  \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7P0izvLJtbEv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 4 superato!\n"
     ]
    }
   ],
   "source": [
    "assert sql_ethereum_df.schema.fieldNames() == ['newCol', 'count'] \n",
    "assert sql_ethereum_df.count() == 3\n",
    "row = sql_ethereum_df.filter(col('newCol') == '0x1').collect()[0]\n",
    "assert row['count'] == 66\n",
    "print(\"Esercizio 4 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnt7nNNzQzRN"
   },
   "source": [
    "# Esercizio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUIhtWjdQ2B-"
   },
   "source": [
    "Dal dataframe `people_df` filtriamo i record con la colonna `gender` uguale a Male. \n",
    "\n",
    "Dopodichè calcoliamo una colonna `count` che contenga la count del numero di record per country e prendiamo la riga con la count maggiore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "aPpN-uIFRoE9"
   },
   "outputs": [],
   "source": [
    "male_df = (people_df.filter(col('gender') == 'Male')\n",
    "                .groupBy('country')\n",
    "                .agg(count(\"id\").alias(\"count\"))\n",
    "                .orderBy(\"count\", ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "nrLKWtWDW70L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 5 superato!\n"
     ]
    }
   ],
   "source": [
    "assert male_df.schema.fieldNames() == ['country', 'count'] \n",
    "row_max =  male_df.collect()[0]\n",
    "assert row_max['country'] == 'China'\n",
    "assert row_max['count'] == 89\n",
    "print(\"Esercizio 5 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOAO20uAW9mo"
   },
   "source": [
    "# Esercizio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYrlIOIqdcnA"
   },
   "source": [
    "Il dataframe `people_df` contiene le informazioni sulle persone.\n",
    "\n",
    "Il dataframe `department_df` contiene l'informazione del dipartimento in cui lavora ogni persona.\n",
    "\n",
    "Ogni persona è individuata univocamente dall'`id` su entrambi i dataframe.\n",
    "\n",
    "Adesso vogliamo arricchire il `people_df` con le informazioni del `department_df` ma mantenendo soltanto gli id delle persone presenti su entrambi i dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "UJXjEIB_XAzc"
   },
   "outputs": [],
   "source": [
    "people_dept_df = people_df.join(department_df, 'id', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "9qqRn0zqfUUD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 6 superato!\n"
     ]
    }
   ],
   "source": [
    "assert people_dept_df.count() == 900\n",
    "assert people_dept_df.schema.fieldNames() == ['id', 'first_name', 'last_name', 'email', 'gender', 'ip_address', 'city', 'country', 'ethereum_address', 'department']\n",
    "print(\"Esercizio 6 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB1t4flHf8Ao"
   },
   "source": [
    "# Esercizio 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogjZar58f-qH"
   },
   "source": [
    "Partendo dal dataframe appena calcolato `people_dept_df` dobbiamo calcolare per ogni department quante persone ci sono.\n",
    "\n",
    "La colonna che conterrà il numero di persone la chiameremo `count`.\n",
    "\n",
    "Prendiamo solo i department che hanno più di 200 persone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "9UmrIBebgLaz"
   },
   "outputs": [],
   "source": [
    "count_people_dept_df = people_dept_df.groupBy('department').agg(count(\"id\").alias(\"count\")).filter(col('count') > 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "wtq5p50zhSd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 7 superato!\n"
     ]
    }
   ],
   "source": [
    "assert count_people_dept_df.count() == 1\n",
    "assert count_people_dept_df.schema.fieldNames() == ['department', 'count']\n",
    "row = count_people_dept_df.collect()[0]\n",
    "assert row['count'] == 203\n",
    "print(\"Esercizio 7 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2D-F8WEjg3X"
   },
   "source": [
    "# Esercizio 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIWT-eHdjmyf"
   },
   "source": [
    "Ripetere i passaggi dell'esercizio 7 ma usando `Spark SQL`\n",
    "\n",
    "Ricordiamo che prima di usare `spark.sql` e fare la query è necessario registrare il dataframe `people_dept_df` col metodo `createOrReplaceTempView`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "jNVjQNTIq6b2"
   },
   "outputs": [],
   "source": [
    "people_dept_df.createOrReplaceTempView(\"people_dept_df\")\n",
    "sql_count_people_dept_df = (spark.sql(\"\"\"\n",
    "                              SELECT\n",
    "                                  department,\n",
    "                                  count(*) AS count\n",
    "                              FROM people_dept_df\n",
    "                              GROUP BY department\n",
    "                              HAVING count > 200\n",
    "                            \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "l6aCPpLiqrBU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 8 superato!\n"
     ]
    }
   ],
   "source": [
    "assert sql_count_people_dept_df.count() == 1\n",
    "assert sql_count_people_dept_df.schema.fieldNames() == ['department', 'count']\n",
    "row = sql_count_people_dept_df.collect()[0]\n",
    "assert row['count'] == 203\n",
    "print(\"Esercizio 8 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIKw6veCPyOt"
   },
   "source": [
    "# Esercizio 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B1C5SfUP1Ji"
   },
   "source": [
    "Da `people_df` filtriamo la colonna `ip_address` prendendo gli indirizzi ip che iniziano per `21`, dopodichè calcoliamo per quale `country` sono presenti più indirizzi ip. (Usiamo `pyspark.sql.Column.like`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "deFa_wcAQaYX"
   },
   "outputs": [],
   "source": [
    "ip_address_per_country_df = (people_df.filter(col('ip_address').like('21%'))\n",
    "        .groupBy('country')\n",
    "        .agg(count(\"ip_address\").alias(\"count\"))\n",
    "        .orderBy('count', ascending = False)\n",
    "        .limit(1))                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "ea4BMtLDRkMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 9 superato!\n"
     ]
    }
   ],
   "source": [
    "assert ip_address_per_country_df.schema.fieldNames() == ['country', 'count']\n",
    "row = ip_address_per_country_df.filter(col('country') == 'Indonesia').collect()[0]\n",
    "assert row['count'] == 7\n",
    "print(\"Esercizio 9 superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcWmM5edT-OS"
   },
   "source": [
    "# Esercizio 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52zWn9j5UAaC"
   },
   "source": [
    "Da `people_df` filtriamo la colonna `email` considerando solo i valori che finiscono per `.com`. Poi facciamo una count aggregata per genere e estraiamo solo la prima riga contenente il numero maggiore di count.\n",
    "(Usiamo `pyspark.sql.Column.rlike`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "N3mDEgX2U30d"
   },
   "outputs": [],
   "source": [
    "email_per_gender_df = (people_df.filter(col('email').rlike('.com$'))\n",
    "                        .groupBy('gender')\n",
    "                        .agg(count(\"ip_address\").alias(\"count\"))\n",
    "                        .orderBy('count', ascending = False)\n",
    "                        .limit(1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "6aggnSJUXghr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esercizio 10 superato!\n"
     ]
    }
   ],
   "source": [
    "assert email_per_gender_df.count() == 1\n",
    "assert email_per_gender_df.schema.fieldNames() == ['gender', 'count']\n",
    "row = email_per_gender_df.collect()[0]\n",
    "assert row['gender'] == 'Female'\n",
    "assert row['count'] == 275\n",
    "print(\"Esercizio 10 superato!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOEjjgtv8Sx2N8H7yoCpias",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
